
@article{trapeau_fast_2016,
	title = {Fast and persistent adaptation to new spectral cues for sound localization suggests a many-to-one mapping mechanism},
	volume = {140},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/140/2/879/663968/Fast-and-persistent-adaptation-to-new-spectral},
	doi = {10.1121/1.4960568},
	abstract = {The adult human auditory system can adapt to changes in spectral cues for sound localization. This plasticity was demonstrated by changing the shape of the pinna with earmolds. Previous results indicate that participants regain localization accuracy after several weeks of adaptation and that the adapted state is retained for at least one week without earmolds. No aftereffect was observed after mold removal, but any aftereffect may be too short to be observed when responses are averaged over many trials. This work investigated the lack of aftereffect by analyzing single-trial responses and modifying visual, auditory, and tactile information during the localization task. Results showed that participants localized accurately immediately after mold removal, even at the first stimulus presentation. Knowledge of the stimulus spectrum, tactile information about the absence of the earmolds, and visual feedback were not necessary to localize accurately after adaptation. Part of the adaptation persisted for one month without molds. The results are consistent with the hypothesis of a many-to-one mapping of the spectral cues, in which several spectral profiles are simultaneously associated with one sound location. Additionally, participants with acoustically more informative spectral cues localized sounds more accurately, and larger acoustical disturbances by the molds reduced adaptation success.},
	language = {en},
	number = {2},
	urldate = {2023-12-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Trapeau, Régis and Aubrais, Valérie and Schönwiesner, Marc},
	month = aug,
	year = {2016},
	pages = {879--890},
	file = {Trapeau et al. - 2016 - Fast and persistent adaptation to new spectral cue.pdf:/Users/paulfriedrich/Zotero/storage/LE5ASMAA/Trapeau et al. - 2016 - Fast and persistent adaptation to new spectral cue.pdf:application/pdf},
}

@article{hofman_relearning_1998,
	title = {Relearning sound localization with new ears},
	volume = {1},
	language = {en},
	number = {5},
	journal = {nature neuroscience},
	author = {Hofman, Paul M},
	year = {1998},
	file = {Hofman - 1998 - Relearning sound localization with new ears.pdf:/Users/paulfriedrich/Zotero/storage/9VNQY4SF/Hofman - 1998 - Relearning sound localization with new ears.pdf:application/pdf},
}

@article{middlebrooks_individual_1999,
	title = {Individual differences in external-ear transfer functions reduced by scaling in frequency},
	volume = {106},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/106/3/1480/553209/Individual-differences-in-external-ear-transfer},
	doi = {10.1121/1.427176},
	abstract = {This study examined inter-subject differences in the transfer functions from the free field to the human ear canal, which are commonly know as head-related transfer functions. The directional components of such transfer functions are referred here to as directional transfer functions (DTFs). The DTFs of 45 subjects varied systematically among subjects in regard to the frequencies of spectral features such as peaks and notches. Inter-subject spectral differences in DTFs were quantified between 3.7 and 12.9 kHz for sound-source directions throughout the coordinate sphere. For each pair of subjects, an optimal frequency scale factor aligned spectral features between subjects and, thus, minimized inter-subject spectral differences. Frequency scaling of DTFs reduced spectral differences by a median value of 15.5\% across all pairs of subjects and by more than half in 9.5\% of subject pairs. Optimal scale factors showed a median value of 1.061 and a maximum of 1.38. The optimal scale factor between any pair of subjects correlated highly with the ratios of subjects’ maximum interaural delays, sizes of their external ears, and widths of their heads.},
	language = {en},
	number = {3},
	urldate = {2023-12-12},
	journal = {The Journal of the Acoustical Society of America},
	author = {Middlebrooks, John C.},
	month = sep,
	year = {1999},
	pages = {1480--1492},
	file = {Middlebrooks - 1999 - Individual differences in external-ear transfer fu.pdf:/Users/paulfriedrich/Zotero/storage/MD4UVQEV/Middlebrooks - 1999 - Individual differences in external-ear transfer fu.pdf:application/pdf},
}

@article{wanrooij_relearning_2005,
	title = {Relearning {Sound} {Localization} with a {New} {Ear}},
	volume = {25},
	copyright = {Copyright © 2005 Society for Neuroscience 0270-6474/05/255413-12.00/0},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/content/25/22/5413},
	doi = {10.1523/JNEUROSCI.0850-05.2005},
	abstract = {Human sound localization results primarily from the processing of binaural differences in sound level and arrival time for locations in the horizontal plane (azimuth) and of spectral shape cues generated by the head and pinnae for positions in the vertical plane (elevation). The latter mechanism incorporates two processing stages: a spectral-to-spatial mapping stage and a binaural weighting stage that determines the contribution of each ear to perceived elevation as function of sound azimuth. We demonstrated recently that binaural pinna molds virtually abolish the ability to localize sound-source elevation, but, after several weeks, subjects regained normal localization performance. It is not clear which processing stage underlies this remarkable plasticity, because the auditory system could have learned the new spectral cues separately for each ear (spatial-mapping adaptation) or for one ear only, while extending its contribution into the contralateral hemifield (binaural-weighting adaptation). To dissociate these possibilities, we applied a long-term monaural spectral perturbation in 13 subjects. Our results show that, in eight experiments, listeners learned to localize accurately with new spectral cues that differed substantially from those provided by their own ears. Interestingly, five subjects, whose spectral cues were not sufficiently perturbed, never yielded stable localization performance. Our findings indicate that the analysis of spectral cues may involve a correlation process between the sensory input and a stored spectral representation of the subject's ears and that learning acts predominantly at a spectral-to-spatial mapping level rather than at the level of binaural weighting.},
	language = {en},
	number = {22},
	urldate = {2023-12-16},
	journal = {Journal of Neuroscience},
	author = {Wanrooij, Marc M. Van and Opstal, A. John Van},
	month = jun,
	year = {2005},
	pmid = {15930391},
	note = {Publisher: Society for Neuroscience
Section: Behavioral/Systems/Cognitive},
	keywords = {directional hearing, human, monaural, pinna, plasticity, spectral cues},
	pages = {5413--5424},
	file = {Full Text PDF:/Users/paulfriedrich/Zotero/storage/XWUAKE23/Wanrooij and Opstal - 2005 - Relearning Sound Localization with a New Ear.pdf:application/pdf},
}

@article{schonwiesner_soundlab_2021,
	title = {s(ound)lab: {An} easy to learn {Python} package for designing and running psychoacoustic experiments.},
	volume = {6},
	issn = {2475-9066},
	shorttitle = {s(ound)lab},
	url = {https://joss.theoj.org/papers/10.21105/joss.03284},
	doi = {10.21105/joss.03284},
	abstract = {Slab enables researchers and students to prototype and implement psychoacoustic experiments quickly. Slab implements many of the procedures for psychoacoustic research and experiment control and is easily combined with other Python software. A secondary aim of slab is to enable researchers and students without prior training in computer programming and digital signal processing to implement and conduct these experiments. Slab provides building blocks rather than ready-made solutions, so that experimenters still need to carefully consider stimulation, sequencing and data management. This also makes slab very flexible and easy to customise. In the documentation (see slab.readthedocs.io), we provide tutorials suitable for beginners. We also provide experiments conducted in our lab as worked examples.},
	language = {en},
	number = {62},
	urldate = {2023-12-29},
	journal = {Journal of Open Source Software},
	author = {Schönwiesner, Marc and Bialas, Ole},
	month = jun,
	year = {2021},
	pages = {3284},
	file = {Schönwiesner and Bialas - 2021 - s(ound)lab An easy to learn Python package for de.pdf:/Users/paulfriedrich/Zotero/storage/2NDZIXJJ/Schönwiesner and Bialas - 2021 - s(ound)lab An easy to learn Python package for de.pdf:application/pdf},
}

@article{ward_stimulus_1979,
	title = {Stimulus {Information} and {Sequential} {Dependencies} in {Magnitude} {Estimation} and {Cross}-{Modality} {Matching}},
	volume = {5},
	language = {en},
	number = {3},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Ward, Lawrence M},
	year = {1979},
	pages = {444--459},
	file = {Ward - Stimulus Information and Sequential Dependencies i.pdf:/Users/paulfriedrich/Zotero/storage/NKVPDAHN/Ward - Stimulus Information and Sequential Dependencies i.pdf:application/pdf},
}

@article{andeol_sound_2013,
	title = {Sound localization in noise and sensitivity to spectral shape},
	volume = {304},
	issn = {03785955},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378595513001445},
	doi = {10.1016/j.heares.2013.06.001},
	abstract = {Individual differences exist in sound localization performance even for normal-hearing listeners. Some of these differences might be related to acoustical differences in localization cues carried by the head related transfer functions (HRTF). Recent data suggest that individual differences in sound localization performance could also have a perceptual origin. The localization of an auditory target in the up/down and front/back dimensions requires the analysis of the spectral shape of the stimulus. In the present study, we investigated the role of an acoustic factor, the prominence of the spectral shape (“spectral strength”) and the role of a perceptual factor, the listener’s sensitivity to spectral shape, in individual differences observed in sound localization performance. Spectral strength was computed as the spectral distance between the magnitude spectrum of the HRTFs and a ﬂat spectrum. Sensitivity to spectral shape was evaluated using spectral-modulation thresholds measured with a broadband (0.2e12.8 kHz) or highfrequency (4e16 kHz) carrier and for different spectral modulation frequencies (below 1 cycle/octave, between 1 and 2 cycles/octave, above 2 cycles/octave). Data obtained from 19 young normal-hearing listeners showed that low thresholds for spectral modulation frequency below 1 cycle/octave with a high-frequency carrier were associated with better sound localization performance. No correlation was found between sound localization performance and the spectral strength of the HRTFs. These results suggest that differences in perceptual ability, rather than acoustical differences, contribute to individual differences in sound localization performance in noise.},
	language = {en},
	urldate = {2023-12-30},
	journal = {Hearing Research},
	author = {Andéol, Guillaume and Macpherson, Ewan A. and Sabin, Andrew T.},
	month = oct,
	year = {2013},
	pages = {20--27},
	file = {Andéol et al. - 2013 - Sound localization in noise and sensitivity to spe.pdf:/Users/paulfriedrich/Zotero/storage/3UC6Z8CL/Andéol et al. - 2013 - Sound localization in noise and sensitivity to spe.pdf:application/pdf},
}

@article{reiss_spectral_2005,
	title = {Spectral {Edge} {Sensitivity} in {Neural} {Circuits} of the {Dorsal} {Cochlear} {Nucleus}},
	volume = {25},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.4963-04.2005},
	doi = {10.1523/JNEUROSCI.4963-04.2005},
	abstract = {One possible function of the dorsal cochlear nucleus (DCN) is discrimination of head-related transfer functions (HRTFs), spectral cues used for vertical sound localization. Recent psychophysical and physiological studies suggest that steep, rising spectral edges may be the features used to identify HRTFs. Here we showed, using notch noise and noise band stimuli presented over a range of frequencies, that a subclass of DCN type IV neurons responded with a response peak when the rising spectral edge of a notch or band was aligned near best frequency (BF). This edge sensitivity was correlated with weak or inhibited responses to broadband noise and inhibition in receptive fields at frequencies below BF. Some aspects of the inhibition shaping the response peak, namely inhibition to rising edges below BF and to falling edges at BF, could be explained by the properties of type II interneurons with BFs below those of the type IV neurons. However, many type IV neurons also showed inhibitory responses with the rising spectral edge just above BF, and these responses could not be reproduced by current models of DCN circuitry. Therefore, a new component of the DCN circuit is needed to fully explain the responses to rising spectral edges. This shaping of edge sensitivity by inhibition to rising spectral edges both below and above BF suggests the specialization of DCN for spectral edge coding along the tonotopic gradient.},
	language = {en},
	number = {14},
	urldate = {2024-01-11},
	journal = {The Journal of Neuroscience},
	author = {Reiss, Lina A. J. and Young, Eric D.},
	month = apr,
	year = {2005},
	pages = {3680--3691},
	file = {Reiss and Young - 2005 - Spectral Edge Sensitivity in Neural Circuits of th.pdf:/Users/paulfriedrich/Zotero/storage/EZDFQZTY/Reiss and Young - 2005 - Spectral Edge Sensitivity in Neural Circuits of th.pdf:application/pdf},
}

@article{langendijk_contribution_2002,
	title = {Contribution of spectral cues to human sound localization},
	volume = {112},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/112/4/1583/550090/Contribution-of-spectral-cues-to-human-sound},
	doi = {10.1121/1.1501901},
	abstract = {The contribution of spectral cues to human sound localization was investigated by removing cues in 12-, 1- or 2-octave bands in the frequency range above 4 kHz. Localization responses were given by placing an acoustic pointer at the same apparent position as a virtual target. The pointer was generated by filtering a 100-ms harmonic complex with equalized head-related transfer functions (HRTFs). Listeners controlled the pointer via a hand-held stick that rotated about a fixed point. In the baseline condition, the target, a 200-ms noise burst, was filtered with the same HRTFs as the pointer. In other conditions, the spectral information within a certain frequency band was removed by replacing the directional transfer function within this band with the average transfer of this band. Analysis of the data showed that removing cues in 12-octave bands did not affect localization, whereas for the 2-octave band correct localization was virtually impossible. The results obtained for the 1-octave bands indicate that up–down cues are located mainly in the 6–12-kHz band, and front–back cues in the 8–16-kHz band. The interindividual spread in response patterns suggests that different listeners use different localization cues. The response patterns in the median plane can be predicted using a model based on spectral comparison of directional transfer functions for target and response directions.},
	language = {en},
	number = {4},
	urldate = {2024-01-13},
	journal = {The Journal of the Acoustical Society of America},
	author = {Langendijk, Erno H. A. and Bronkhorst, Adelbert W.},
	month = oct,
	year = {2002},
	pages = {1583--1596},
	file = {Langendijk and Bronkhorst - 2002 - Contribution of spectral cues to human sound local.pdf:/Users/paulfriedrich/Zotero/storage/ZH3HR2W8/Langendijk and Bronkhorst - 2002 - Contribution of spectral cues to human sound local.pdf:application/pdf},
}

@article{davis_auditory_2003,
	title = {Auditory {Processing} of {Spectral} {Cues} for {Sound} {Localization} in the {Inferior} {Colliculus}},
	volume = {4},
	issn = {1525-3961, 1438-7573},
	url = {http://link.springer.com/10.1007/s10162-002-2002-5},
	doi = {10.1007/s10162-002-2002-5},
	abstract = {The head-related transfer function (HRTF) of the cat adds directionally dependent energy minima to the amplitude spectrum of complex sounds. These spectral notches are a principal cue for the localization of sound source elevation. Physiological evidence suggests that the dorsal cochlear nucleus (DCN) plays a critical role in the brainstem processing of this directional feature. Type O units in the central nucleus of the inferior colliculus (ICC) are a primary target of ascending DCN projections and, therefore, may represent midbrain specializations for the auditory processing of spectral cues for sound localization. Behavioral studies conﬁrm a loss of sound orientation accuracy when DCN projections to the inferior colliculus are surgically lesioned. This study used simple analogs of HRTF notches to characterize single-unit response patterns in the ICC of decerebrate cats that may contribute to the directional sensitivity of the brain’s spectral processing pathways. Manipulations of notch frequency and bandwidth demonstrated frequency-speciﬁc excitatory responses that have the capacity to encode HRTF-based cues for sound source location. These response patterns were limited to type O units in the ICC and have not been observed for the projection neurons of the DCN. The unique spectral integration properties of type O units suggest that DCN inﬂuences are transformed into a more selective representation of sound source location by a local convergence of wideband excitatory and frequency-tuned inhibitory inputs.},
	language = {en},
	number = {2},
	urldate = {2024-01-13},
	journal = {JARO - Journal of the Association for Research in Otolaryngology},
	author = {Davis, Kevin A. and Ramachandran, Ramnarayan and May, Bradford J.},
	month = jun,
	year = {2003},
	pages = {148--163},
	file = {Davis et al. - 2003 - Auditory Processing of Spectral Cues for Sound Loc.pdf:/Users/paulfriedrich/Zotero/storage/6PAJNQMK/Davis et al. - 2003 - Auditory Processing of Spectral Cues for Sound Loc.pdf:application/pdf},
}

@article{algazi_elevation_2001,
	title = {Elevation localization and head-related transfer function analysis at low frequencies},
	volume = {109},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/109/3/1110/553520/Elevation-localization-and-head-related-transfer},
	doi = {10.1121/1.1349185},
	abstract = {Monaural spectral features due to pinna diffraction are the primary cues for elevation. Because these features appear above 3 kHz where the wavelength becomes comparable to pinna size, it is generally believed that accurate elevation estimation requires wideband sources. However, psychoacoustic tests show that subjects can estimate elevation for low-frequency sources. In the experiments reported, random noise bursts low-pass filtered to 3 kHz were processed with individualized head-related transfer functions (HRTFs), and six subjects were asked to report the elevation angle around four cones of confusion. The accuracy in estimating elevation was degraded when compared to a baseline test with wideband stimuli. The reduction in performance was a function of azimuth and was highest in the median plane. However, when the source was located away from the median plane, subjects were able to estimate elevation, often with surprisingly good accuracy. Analysis of the HRTFs reveals the existence of elevation-dependent features at low frequencies. The physical origin of the low-frequency features is attributed primarily to head diffraction and torso reflections. It is shown that simple geometrical approximations and models of the head and torso explain these low-frequency features and the corresponding elevations cues.},
	language = {en},
	number = {3},
	urldate = {2024-01-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Algazi, V. Ralph and Avendano, Carlos and Duda, Richard O.},
	month = mar,
	year = {2001},
	pages = {1110--1122},
	file = {Algazi et al. - 2001 - Elevation localization and head-related transfer f.pdf:/Users/paulfriedrich/Zotero/storage/KBJV8XWX/Algazi et al. - 2001 - Elevation localization and head-related transfer f.pdf:application/pdf},
}

@article{asano_role_1990,
	title = {Role of spectral cues in median plane localization},
	volume = {88},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/88/1/159/626776/Role-of-spectral-cues-in-median-plane},
	doi = {10.1121/1.399963},
	abstract = {The role of spectral cues in the sound source to ear transfer function in median plane sound localization is investigated in this paper. At first, transfer functions were measured and analyzed. Then, these transfer functions were used in experiments where sounds from a source on the median plane were simulated and presented to subjects through headphones. In these simulation experiments, the transfer functions were smoothed by ARMA models with different degrees of simplification to investigate the role of microscopic and macroscopic patterns in the transfer functions for median plane localization. The results of the study are summarized as follows: (1) For front–rear judgment, information derived from microscopic peaks and dips in the low-frequency region (below 2 kHz) and the macroscopic patterns in the high-frequency region seems to be utilized; (2) for judgment of elevation angle, major cues exist in the high-frequency region above 5 kHz. The information in macroscopic patterns is utilized instead of that in small peaks and dips.},
	language = {en},
	number = {1},
	urldate = {2024-01-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Asano, Futoshi and Suzuki, Yoiti and Sone, Toshio},
	month = jul,
	year = {1990},
	pages = {159--168},
	file = {Asano et al. - 1990 - Role of spectral cues in median plane localization.pdf:/Users/paulfriedrich/Zotero/storage/2Q9XQ8PW/Asano et al. - 1990 - Role of spectral cues in median plane localization.pdf:application/pdf},
}

@article{carlile_relearning_2014,
	title = {Relearning {Auditory} {Spectral} {Cues} for {Locations} {Inside} and {Outside} the {Visual} {Field}},
	volume = {15},
	issn = {1525-3961, 1438-7573},
	url = {http://link.springer.com/10.1007/s10162-013-0429-5},
	doi = {10.1007/s10162-013-0429-5},
	language = {en},
	number = {2},
	urldate = {2024-01-18},
	journal = {Journal of the Association for Research in Otolaryngology},
	author = {Carlile, Simon and Blackman, Toby},
	month = apr,
	year = {2014},
	pages = {249--263},
	file = {Carlile and Blackman - 2014 - Relearning Auditory Spectral Cues for Locations In.pdf:/Users/paulfriedrich/Zotero/storage/G65CWZWA/Carlile and Blackman - 2014 - Relearning Auditory Spectral Cues for Locations In.pdf:application/pdf},
}

@article{hofman_spectro-temporal_1998,
	title = {Spectro-temporal factors in two-dimensional human sound localization},
	volume = {103},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/103/5/2634/557650/Spectro-temporal-factors-in-two-dimensional-human},
	doi = {10.1121/1.422784},
	abstract = {This paper describes the effect of spectro-temporal factors on human sound localization performance in two dimensions (2D). Subjects responded with saccadic eye movements to acoustic stimuli presented in the frontal hemisphere. Both the horizontal (azimuth) and vertical (elevation) stimulus location were varied randomly. Three types of stimuli were used, having different spectro-temporal patterns, but identically shaped broadband averaged power spectra: noise bursts, frequency-modulated tones, and trains of short noise bursts. In all subjects, the elevation components of the saccadic responses varied systematically with the different temporal parameters, whereas the azimuth response components remained equally accurate for all stimulus conditions. The data show that the auditory system does not calculate a final elevation estimate from a long-term (order 100 ms) integration of sensory input. Instead, the results suggest that the auditory system may apply a “multiple-look” strategy in which the final estimate is calculated from consecutive short-term (order few ms) estimates. These findings are incorporated in a conceptual model that accounts for the data and proposes a scheme for the temporal processing of spectral sensory information into a dynamic estimate of sound elevation.},
	language = {en},
	number = {5},
	urldate = {2024-01-18},
	journal = {The Journal of the Acoustical Society of America},
	author = {Hofman, Paul M. and Van Opstal, A. John},
	month = may,
	year = {1998},
	pages = {2634--2648},
	file = {Hofman and Van Opstal - 1998 - Spectro-temporal factors in two-dimensional human .pdf:/Users/paulfriedrich/Zotero/storage/3X92HS68/Hofman and Van Opstal - 1998 - Spectro-temporal factors in two-dimensional human .pdf:application/pdf},
}

@article{davis_auditory_2003-1,
	title = {Auditory {Processing} of {Spectral} {Cues} for {Sound} {Localization} in the {Inferior} {Colliculus}},
	volume = {4},
	issn = {1525-3961, 1438-7573},
	url = {http://link.springer.com/10.1007/s10162-002-2002-5},
	doi = {10.1007/s10162-002-2002-5},
	abstract = {The head-related transfer function (HRTF) of the cat adds directionally dependent energy minima to the amplitude spectrum of complex sounds. These spectral notches are a principal cue for the localization of sound source elevation. Physiological evidence suggests that the dorsal cochlear nucleus (DCN) plays a critical role in the brainstem processing of this directional feature. Type O units in the central nucleus of the inferior colliculus (ICC) are a primary target of ascending DCN projections and, therefore, may represent midbrain specializations for the auditory processing of spectral cues for sound localization. Behavioral studies conﬁrm a loss of sound orientation accuracy when DCN projections to the inferior colliculus are surgically lesioned. This study used simple analogs of HRTF notches to characterize single-unit response patterns in the ICC of decerebrate cats that may contribute to the directional sensitivity of the brain’s spectral processing pathways. Manipulations of notch frequency and bandwidth demonstrated frequency-speciﬁc excitatory responses that have the capacity to encode HRTF-based cues for sound source location. These response patterns were limited to type O units in the ICC and have not been observed for the projection neurons of the DCN. The unique spectral integration properties of type O units suggest that DCN inﬂuences are transformed into a more selective representation of sound source location by a local convergence of wideband excitatory and frequency-tuned inhibitory inputs.},
	language = {en},
	number = {2},
	urldate = {2024-01-18},
	journal = {JARO - Journal of the Association for Research in Otolaryngology},
	author = {Davis, Kevin A. and Ramachandran, Ramnarayan and May, Bradford J.},
	month = jun,
	year = {2003},
	pages = {148--163},
	file = {Davis et al. - 2003 - Auditory Processing of Spectral Cues for Sound Loc.pdf:/Users/paulfriedrich/Zotero/storage/ZDBJKIPI/Davis et al. - 2003 - Auditory Processing of Spectral Cues for Sound Loc.pdf:application/pdf},
}

@article{king_spatial_1987,
	title = {Spatial response properties of acoustically responsive neurons in the superior colliculus of the ferret: a map of auditory space},
	volume = {57},
	issn = {0022-3077, 1522-1598},
	shorttitle = {Spatial response properties of acoustically responsive neurons in the superior colliculus of the ferret},
	url = {https://www.physiology.org/doi/10.1152/jn.1987.57.2.596},
	doi = {10.1152/jn.1987.57.2.596},
	abstract = {Extracellular single-unit recordings were made from auditory neurons in the superior colliculus of ferrets anesthetized with either a neuroleptic or a combination of barbiturate with paralysis. The response properties of these neurons were studied using white-noise bursts presented under free-field conditions in an anechoic chamber. Auditory neurons were found throughout the intermediate and deep layers of the superior colliculus. All neurons were spontaneously active, the rates of discharge varying from 0.1 to 61.1 spikes X s-1. Although the spontaneous discharge interspike-interval histograms for many units approximated to exponential distributions, the histograms of 44\% had clear secondary peaks, indicating more than one preferred interval, and could not be modeled by a simple process. Most neurons (50\%) responded only at stimulus onset, whereas 12\% exhibited sustained discharges and 38\% gave onset responses followed by a period of silence or reduced activity and then a period of elevated discharge, which was not apparently related to stimulus offset. Neurons with multipeaked response patterns were concentrated in the stratum griseum profundum. The latencies from arrival of the stimulus at the ear to the onset of neural activity ranged from 6 to 49 ms and decreased with increasing stimulus intensity. Although responsive to sounds over a large region of space, most neurons had clearly defined best positions at which the strongest response was obtained. The response declined as the speaker was moved away from this position, and nearly all units had peaked response profiles. The spatial tuning varied between different neurons, but most were more sharply tuned in elevation than in azimuth. Increasing the stimulus intensity did not, in general, alter the best positions of these neurons, but usually resulted in a broadening of the receptive fields, although other units became more sharply tuned. The best positions of auditory neurons varied systematically in azimuth from 20 degrees into the ipsilateral hemifield to 130 degrees into the contralateral hemifield as the electrode was moved from the rostrolateral to the caudomedial end of the superior colliculus. The best positions shifted in elevation along a rostromedial to caudolateral axis from 60 degrees above to 50 degrees below the visuoaural plane.(ABSTRACT TRUNCATED AT 400 WORDS)},
	language = {en},
	number = {2},
	urldate = {2024-01-18},
	journal = {Journal of Neurophysiology},
	author = {King, A. J. and Hutchings, M. E.},
	month = feb,
	year = {1987},
	pages = {596--624},
	file = {King and Hutchings - 1987 - Spatial response properties of acoustically respon.pdf:/Users/paulfriedrich/Zotero/storage/RYBX9HVD/King and Hutchings - 1987 - Spatial response properties of acoustically respon.pdf:application/pdf},
}

@article{trapeau_encoding_2018,
	title = {The {Encoding} of {Sound} {Source} {Elevation} in the {Human} {Auditory} {Cortex}},
	volume = {38},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.2530-17.2018},
	doi = {10.1523/JNEUROSCI.2530-17.2018},
	abstract = {Spatial hearing is a crucial capacity of the auditory system. While the encoding of horizontal sound direction has been extensively studied, very little is known about the representation of vertical sound direction in the auditory cortex. Using high-resolution fMRI, we measured voxelwise sound elevation tuning curves in human auditory cortex and show that sound elevation is represented by broad tuning functions preferring lower elevations as well as secondary narrow tuning functions preferring individual elevation directions. We changed the ear shape of participants (male and female) with silicone molds for several days. This manipulation reduced or abolished the ability to discriminate sound elevation and flattened cortical tuning curves. Tuning curves recovered their original shape as participants adapted to the modified ears and regained elevation perception over time. These findings suggest that the elevation tuning observed in low-level auditory cortex did not arise from the physical features of the stimuli but is contingent on experience with spectral cues and covaries with the change in perception. One explanation for this observation may be that the tuning in low-level auditory cortex underlies the subjective perception of sound elevation.
            
              SIGNIFICANCE STATEMENT
              This study addresses two fundamental questions about the brain representation of sensory stimuli: how the vertical spatial axis of auditory space is represented in the auditory cortex and whether low-level sensory cortex represents physical stimulus features or subjective perceptual attributes. Using high-resolution fMRI, we show that vertical sound direction is represented by broad tuning functions preferring lower elevations as well as secondary narrow tuning functions preferring individual elevation directions. In addition, we demonstrate that the shape of these tuning functions is contingent on experience with spectral cues and covaries with the change in perception, which may indicate that the tuning functions in low-level auditory cortex underlie the perceived elevation of a sound source.},
	language = {en},
	number = {13},
	urldate = {2024-01-18},
	journal = {The Journal of Neuroscience},
	author = {Trapeau, Régis and Schönwiesner, Marc},
	month = mar,
	year = {2018},
	pages = {3252--3264},
	file = {Trapeau and Schönwiesner - 2018 - The Encoding of Sound Source Elevation in the Huma.pdf:/Users/paulfriedrich/Zotero/storage/PQEIDDTX/Trapeau and Schönwiesner - 2018 - The Encoding of Sound Source Elevation in the Huma.pdf:application/pdf},
}

@article{otte_age-related_2013,
	title = {Age-related {Hearing} {Loss} and {Ear} {Morphology} {Affect} {Vertical} but not {Horizontal} {Sound}-{Localization} {Performance}},
	volume = {14},
	issn = {1525-3961, 1438-7573},
	url = {http://link.springer.com/10.1007/s10162-012-0367-7},
	doi = {10.1007/s10162-012-0367-7},
	language = {en},
	number = {2},
	urldate = {2024-01-22},
	journal = {Journal of the Association for Research in Otolaryngology},
	author = {Otte, Rik J. and Agterberg, Martijn J. H. and Van Wanrooij, Marc M. and Snik, Ad F. M. and Van Opstal, A. John},
	month = apr,
	year = {2013},
	pages = {261--273},
	file = {Otte et al. - 2013 - Age-related Hearing Loss and Ear Morphology Affect.pdf:/Users/paulfriedrich/Zotero/storage/DTNUGHFZ/Otte et al. - 2013 - Age-related Hearing Loss and Ear Morphology Affect.pdf:application/pdf},
}

@article{clifton_growth_1988,
	title = {Growth in {Head} {Size} {During} {Infancy}: {Implications} for {Sound} {Localization}},
	volume = {24},
	copyright = {Copyright 1988 by the American Psychological Association, Inc},
	doi = {0O12-1649/88/$00.75},
	abstract = {We measured head circumference and interaural distance in infants between birth and 22 weeks of age. A small sample of preschool children and adults were measured for comparison over the life span. We used these data to calculate changing interaural time differences across ages. Large shifts in this important binaural cue suggest that an ongoing developmental process recalibrates the association between interaural time differences and spatial location. These new data confirmed the sex differences in head circumference described in the Berkeley Growth Study (Eichorn \& Bailey, 1962) and found no secular trend in this measure in the 60 years since the earlier data were collected.},
	language = {en},
	number = {4},
	journal = {Developmental Psychology},
	author = {Clifton, Rachel K and Clarkson, Marsha G and Gwiazda, Jane and Bauer, Joseph A},
	year = {1988},
	pages = {477483},
	file = {Clifton et al. - Growth in Head Size During Infancy Implications f.pdf:/Users/paulfriedrich/Zotero/storage/SIBKWVXM/Clifton et al. - Growth in Head Size During Infancy Implications f.pdf:application/pdf},
}

@article{wightman_headphone_1989,
	title = {Headphone simulation of free-field listening. {II}: {Psychophysical} validation},
	volume = {85},
	issn = {0001-4966, 1520-8524},
	shorttitle = {Headphone simulation of free-field listening. {II}},
	url = {https://pubs.aip.org/jasa/article/85/2/868/807017/Headphone-simulation-of-free-field-listening-II},
	doi = {10.1121/1.397558},
	abstract = {Listeners reported the apparent spatial positions of wideband noise bursts that were presented either by loudspeakers in free field or by headphones. The headphone stimuli were digitally processed with the aim of duplicating, at a listener’s eardrums, the waveforms that were produced by the free-field stimuli. The processing algorithms were based on each subject’s free-field-to-eardrum transfer functions that had been measured at 144 free-field source locations. The headphone stimuli were localized by eight subjects in virtually the same positions as the corresponding free-field stimuli. However, with headphone stimuli, there were more front–back confusions, and source elevation seemed slightly less well defined. One subject’s difficulty with elevation judgments, which was observed both with free-field and with headphone stimuli, was traced to distorted features of the free-field-to-eardrum transfer function.},
	language = {en},
	number = {2},
	urldate = {2024-01-22},
	journal = {The Journal of the Acoustical Society of America},
	author = {Wightman, Frederic L. and Kistler, Doris J.},
	month = feb,
	year = {1989},
	pages = {868--878},
	file = {Wightman and Kistler - 1989 - Headphone simulation of free-field listening. II .pdf:/Users/paulfriedrich/Zotero/storage/P572F5W5/Wightman and Kistler - 1989 - Headphone simulation of free-field listening. II .pdf:application/pdf},
}

@book{blauert_spatial_1996,
	title = {Spatial {Hearing}: {The} {Psychophysics} of {Human} {Sound} {Localization}},
	isbn = {978-0-262-26868-4},
	url = {https://doi.org/10.7551/mitpress/6391.001.0001},
	abstract = {The field of spatial hearing has exploded in the decade or so since Jens Blauert's classic work on acoustics was first published in English. This revised edition adds a new chapter that describes developments in such areas as auditory virtual reality (an important field of application that is based mainly on the physics of spatial hearing), binaural technology (modeling speech enhancement by binaural hearing), and spatial sound-field mapping. The chapter also includes recent research on the precedence effect that provides clear experimental evidence that cognition plays a significant role in spatial hearing. The remaining four chapters in this comprehensive reference cover auditory research procedures and psychometric methods, spatial hearing with one sound source, spatial hearing with multiple sound sources and in enclosed spaces, and progress and trends from 1972 (the first German edition) to 1983 (the first English edition)—work that includes research on the physics of the external ear, and the application of signal processing theory to modeling the spatial hearing process. There is an extensive bibliography of more than 900 items.},
	publisher = {The MIT Press},
	author = {Blauert, Jens},
	month = oct,
	year = {1996},
	doi = {10.7551/mitpress/6391.001.0001},
}

@article{parseihian_rapid_2012,
	title = {Rapid head-related transfer function adaptation using a virtual auditory environment},
	volume = {131},
	issn = {0001-4966, 1520-8524},
	url = {https://pubs.aip.org/jasa/article/131/4/2948/831116/Rapid-head-related-transfer-function-adaptation},
	doi = {10.1121/1.3687448},
	abstract = {The paper reports on the ability of people to rapidly adapt in localizing virtual sound sources in both azimuth and elevation when listening to sounds synthesized using non-individualized head-related transfer functions (HRTFs). Participants were placed within an audio-kinesthetic Virtual Auditory Environment (VAE) platform that allows association of the physical position of a virtual sound source with an alternate set of acoustic spectral cues through the use of a tracked physical ball manipulated by the subject. This set-up offers a natural perception-action coupling, which is not limited to the visual field of view. The experiment consisted of three sessions: an initial localization test to evaluate participants’ performance, an adaptation session, and a subsequent localization test. A reference control group was included using individual measured HRTFs. Results show significant improvement in localization performance. Relative to the control group, participants using non-individual HRTFs reduced localization errors in elevation by 10° with three sessions of 12 min. No significant improvement was found for azimuthal errors or for single session adaptation.},
	language = {en},
	number = {4},
	urldate = {2024-01-22},
	journal = {The Journal of the Acoustical Society of America},
	author = {Parseihian, Gaëtan and Katz, Brian F. G.},
	month = apr,
	year = {2012},
	pages = {2948--2957},
	file = {Parseihian and Katz - 2012 - Rapid head-related transfer function adaptation us.pdf:/Users/paulfriedrich/Zotero/storage/3P5K65D9/Parseihian and Katz - 2012 - Rapid head-related transfer function adaptation us.pdf:application/pdf},
}

@article{carlile_plastic_2014,
	title = {The plastic ear and perceptual relearning in auditory spatial perception},
	volume = {8},
	issn = {1662-453X},
	url = {http://journal.frontiersin.org/article/10.3389/fnins.2014.00237/abstract},
	doi = {10.3389/fnins.2014.00237},
	abstract = {The auditory system of adult listeners has been shown to accommodate to altered spectral cues to sound location which presumably provides the basis for recalibration to changes in the shape of the ear over a life time. Here we review the role of auditory and non-auditory inputs to the perception of sound location and consider a range of recent experiments looking at the role of non-auditory inputs in the process of accommodation to these altered spectral cues. A number of studies have used small ear molds to modify the spectral cues that result in signiﬁcant degradation in localization performance. Following chronic exposure (10–60 days) performance recovers to some extent and recent work has demonstrated that this occurs for both audio-visual and audio-only regions of space. This begs the questions as to the teacher signal for this remarkable functional plasticity in the adult nervous system. Following a brief review of inﬂuence of the motor state in auditory localization, we consider the potential role of auditory-motor learning in the perceptual recalibration of the spectral cues. Several recent studies have considered how multi-modal and sensory-motor feedback might inﬂuence accommodation to altered spectral cues produced by ear molds or through virtual auditory space stimulation using non-individualized spectral cues. The work with ear molds demonstrates that a relatively short period of training involving audio-motor feedback (5–10 days) signiﬁcantly improved both the rate and extent of accommodation to altered spectral cues. This has signiﬁcant implications not only for the mechanisms by which this complex sensory information is encoded to provide spatial cues but also for adaptive training to altered auditory inputs. The review concludes by considering the implications for rehabilitative training with hearing aids and cochlear prosthesis.},
	language = {en},
	urldate = {2024-01-24},
	journal = {Frontiers in Neuroscience},
	author = {Carlile, Simon},
	month = aug,
	year = {2014},
	file = {Carlile - 2014 - The plastic ear and perceptual relearning in audit.pdf:/Users/paulfriedrich/Zotero/storage/MIH2IMFQ/Carlile - 2014 - The plastic ear and perceptual relearning in audit.pdf:application/pdf},
}

@article{honda_transfer_2007,
	title = {Transfer effects on sound localization performances from playing a virtual three-dimensional auditory game},
	volume = {68},
	issn = {0003682X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X06001824},
	doi = {10.1016/j.apacoust.2006.08.007},
	abstract = {Transfer eﬀects of playing an auditory game with a virtual auditory display (VAD) were investigated. Furthermore, we analyzed the eﬀects of playing the VAD game on sound localization performance under subjects’ own head-related transfer functions (HRTFs) and HRTFs ﬁtted from those of 16 other adults. Participants performed sound localization tasks initially and 2 weeks later to show the eﬀects. The VAD game players were of three groups, using own HRTFs, ﬁtted HRTFs, and no playing (control). The VAD game-playing results revealed that: (1) the hit rate of the sound localization task for real sound sources increased approximately 20\%; (2) the vertical and horizontal localization error decreased signiﬁcantly; (3) sound localization performance using ﬁtted HRTFs was similar to performance using own HRTFs. Follow-up tests revealed that transfer eﬀects persisted more than 1 month, suggesting that the eﬀects of playing the VAD game transfer to sound localization performance.},
	language = {en},
	number = {8},
	urldate = {2024-01-24},
	journal = {Applied Acoustics},
	author = {Honda, Akio and Shibata, Hiroshi and Gyoba, Jiro and Saitou, Kouji and Iwaya, Yukio and Suzuki, Yôiti},
	month = aug,
	year = {2007},
	pages = {885--896},
	file = {Honda et al. - 2007 - Transfer effects on sound localization performance.pdf:/Users/paulfriedrich/Zotero/storage/2FHIZG6N/Honda et al. - 2007 - Transfer effects on sound localization performance.pdf:application/pdf},
}

@article{steadman_short-term_2019,
	title = {Short-term effects of sound localization training in virtual reality},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-54811-w},
	doi = {10.1038/s41598-019-54811-w},
	abstract = {Abstract
            Head-related transfer functions (HRTFs) capture the direction-dependant way that sound interacts with the head and torso. In virtual audio systems, which aim to emulate these effects, non-individualized, generic HRTFs are typically used leading to an inaccurate perception of virtual sound location. Training has the potential to exploit the brain’s ability to adapt to these unfamiliar cues. In this study, three virtual sound localization training paradigms were evaluated; one provided simple visual positional confirmation of sound source location, a second introduced game design elements (“gamification”) and a final version additionally utilized head-tracking to provide listeners with experience of relative sound source motion (“active listening”). The results demonstrate a significant effect of training after a small number of short (12-minute) training sessions, which is retained across multiple days. Gamification alone had no significant effect on the efficacy of the training, but active listening resulted in a significantly greater improvements in localization accuracy. In general, improvements in virtual sound localization following training generalized to a second set of non-individualized HRTFs, although some HRTF-specific changes were observed in polar angle judgement for the active listening group. The implications of this on the putative mechanisms of the adaptation process are discussed.},
	language = {en},
	number = {1},
	urldate = {2024-01-24},
	journal = {Scientific Reports},
	author = {Steadman, Mark A. and Kim, Chungeun and Lestang, Jean-Hugues and Goodman, Dan F. M. and Picinali, Lorenzo},
	month = dec,
	year = {2019},
	pages = {18284},
	file = {Steadman et al. - 2019 - Short-term effects of sound localization training .pdf:/Users/paulfriedrich/Zotero/storage/NPL5MWAD/Steadman et al. - 2019 - Short-term effects of sound localization training .pdf:application/pdf},
}

@article{steadman_short-term_2019-1,
	title = {Short-term effects of sound localization training in virtual reality},
	volume = {9},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-54811-w},
	doi = {10.1038/s41598-019-54811-w},
	abstract = {Abstract
            Head-related transfer functions (HRTFs) capture the direction-dependant way that sound interacts with the head and torso. In virtual audio systems, which aim to emulate these effects, non-individualized, generic HRTFs are typically used leading to an inaccurate perception of virtual sound location. Training has the potential to exploit the brain’s ability to adapt to these unfamiliar cues. In this study, three virtual sound localization training paradigms were evaluated; one provided simple visual positional confirmation of sound source location, a second introduced game design elements (“gamification”) and a final version additionally utilized head-tracking to provide listeners with experience of relative sound source motion (“active listening”). The results demonstrate a significant effect of training after a small number of short (12-minute) training sessions, which is retained across multiple days. Gamification alone had no significant effect on the efficacy of the training, but active listening resulted in a significantly greater improvements in localization accuracy. In general, improvements in virtual sound localization following training generalized to a second set of non-individualized HRTFs, although some HRTF-specific changes were observed in polar angle judgement for the active listening group. The implications of this on the putative mechanisms of the adaptation process are discussed.},
	language = {en},
	number = {1},
	urldate = {2024-01-24},
	journal = {Scientific Reports},
	author = {Steadman, Mark A. and Kim, Chungeun and Lestang, Jean-Hugues and Goodman, Dan F. M. and Picinali, Lorenzo},
	month = dec,
	year = {2019},
	pages = {18284},
	file = {Steadman et al. - 2019 - Short-term effects of sound localization training .pdf:/Users/paulfriedrich/Zotero/storage/489HLTRK/Steadman et al. - 2019 - Short-term effects of sound localization training .pdf:application/pdf},
}
